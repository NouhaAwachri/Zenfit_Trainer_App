# -*- coding: utf-8 -*-
"""Exercices_Scrapper.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DzuqbYmIHs2HUQa--qgePWpj2I3TQUwF

# Libraries import and installations
"""
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from bs4 import BeautifulSoup
import pandas as pd
import time

import tempfile
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

# Create a unique temporary directory
temp_dir = tempfile.mkdtemp()

options = Options()
options.add_argument("--headless=new")  # Use "new" mode for newer Chrome
options.add_argument(f"--user-data-dir={temp_dir}")
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")

driver = webdriver.Chrome(options=options)

time.sleep(5)

"""# Exercices by muscle Scrapper"""

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
import time
import pandas as pd
import tempfile

options = Options()
options.add_argument("--headless=new")
options.add_argument(f"--user-data-dir={tempfile.mkdtemp()}")
options.add_argument("--no-sandbox")
options.add_argument("--disable-dev-shm-usage")

driver = webdriver.Chrome(options=options)
driver.get("https://www.jefit.com/exercises")
time.sleep(5)

# Define muscle group names exactly as they appear on the site
muscle_names = ["Abs", "Back", "Biceps","Cardio","Chest","Forearms","Glutes", "Shoulders","Triceps", "Upper Legs", "Lower Legs"]

all_exercises = []
seen_links = set()

for muscle_name in muscle_names:
    try:
        # Re-locate buttons in each loop (DOM reload)
        buttons = driver.find_elements(By.XPATH, "//button[.//p]")

        # Click the button with matching <p> text
        target = None
        for b in buttons:
            try:
                text = b.find_element(By.TAG_NAME, "p").text.strip()
                if text == muscle_name:
                    target = b
                    break
            except:
                continue

        if not target:
            print(f"‚ùå Muscle group '{muscle_name}' not found!")
            continue

        print(f"\n‚û°Ô∏è Clicking: {muscle_name}")
        target.click()
        time.sleep(3)

        # Scroll to load all exercises
        for _ in range(11):
            driver.execute_script("window.scrollBy(0, document.body.scrollHeight);")
            time.sleep(2)

        # Get all exercise cards
        cards = driver.find_elements(By.CSS_SELECTOR, "a[href*='/exercises/']")
        print(f"  ‚úÖ Found {len(cards)} cards for {muscle_name}")

        for card in cards:
            try:
                name = card.text.strip()
                href = card.get_attribute("href")
                if name and href and href not in seen_links:
                    all_exercises.append({
                        "muscle_group": muscle_name,
                        "name": name,
                        "link": href
                    })
                    seen_links.add(href)
            except Exception as e:
                print("  ‚ö†Ô∏è Error reading card:", e)

        # Click again to deselect
        target.click()
        time.sleep(1)

    except Exception as e:
        print(f"  ‚ùå Error with '{muscle_name}': {e}")

driver.quit()

# Save to CSV
df = pd.DataFrame(all_exercises)
df.to_csv("jefit_exercises_by_muscle_v2.csv", index=False)
print(f"\n‚úÖ DONE. Saved {len(df)} exercises across {len(muscle_names)} muscle groups.")

"""# Cleaning and saving Exercices by muscles"""

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
import pandas as pd
import time
import tempfile


class JefitScraper:
    def __init__(self):
        self.driver = self._init_driver()
        self.base_url = "https://www.jefit.com/exercises"
        self.muscle_groups = [
            "Abs", "Back", "Biceps", "Cardio", "Chest", "Forearms",
            "Glutes", "Shoulders", "Triceps", "Upper Legs", "Lower Legs"
        ]
        self.exercises = []
        self.seen_links = set()

    def _init_driver(self):
        options = Options()
        options.add_argument("--headless=new")
        options.add_argument(f"--user-data-dir={tempfile.mkdtemp()}")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        return webdriver.Chrome(options=options)

    def scrape(self):
        self.driver.get(self.base_url)
        time.sleep(5)

        for muscle in self.muscle_groups:
            self._scrape_muscle_group(muscle)

        self.driver.quit()

    def _scrape_muscle_group(self, muscle_name):
        try:
            buttons = self.driver.find_elements(By.XPATH, "//button[.//p]")
            target_button = None

            for b in buttons:
                try:
                    if b.find_element(By.TAG_NAME, "p").text.strip() == muscle_name:
                        target_button = b
                        break
                except:
                    continue

            if not target_button:
                print(f"‚ùå Muscle group '{muscle_name}' not found.")
                return

            print(f"\n‚û°Ô∏è Scraping exercises for: {muscle_name}")
            target_button.click()
            time.sleep(3)

            for _ in range(12):
                self.driver.execute_script("window.scrollBy(0, document.body.scrollHeight);")
                time.sleep(2)

            cards = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='/exercises/']")
            print(f"  ‚úÖ Found {len(cards)} cards.")

            for card in cards:
                try:
                    name_raw = card.text.strip()
                    name = " ".join(name_raw.split())
                    link = card.get_attribute("href")
                    exercise_id = link.rstrip('/').split('/')[-1]

                    if link and link not in self.seen_links:
                        self.exercises.append({
                            "muscle_group": muscle_name,
                            "exercise_id": exercise_id,
                            "name": name,
                            "link": link
                        })
                        self.seen_links.add(link)
                except Exception as e:
                    print(f"  ‚ö†Ô∏è Error parsing card: {e}")

            target_button.click()
            time.sleep(1)

        except Exception as e:
            print(f"‚ùå Error scraping '{muscle_name}': {e}")

    def to_csv(self, filepath="jefit_exercises.csv"):
        df = pd.DataFrame(self.exercises)
        df['name'] = df['name'].str.replace(r'\s+', ' ', regex=True).str.strip()
        df.drop_duplicates(subset='link', inplace=True)
        df.to_csv(filepath, index=False)
        print(f"\n‚úÖ Saved {len(df)} exercises to '{filepath}'.")


# ---- Main execution ----
if __name__ == "__main__":
    scraper = JefitScraper()
    scraper.scrape()
    scraper.to_csv("jefit_exercises_by_muscle_cleaned.csv")

"""# Exercices by equipements scrapper"""

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
import time
import pandas as pd
import tempfile

class JefitScraper:
    def __init__(self, equipment_names=None):
        self.equipment_names = equipment_names or [
            "Body Weight", "Bands", "Bench", "Dumbbell", "Exercise Ball", "EZ Curl Bar",
            "Foam Roll", "Barbell", "Kettlebell", "Cardio Machine", "Strength Machine",
            "Pullup Bar", "Weight Plate"
        ]
        self.driver = self._init_driver()
        self.base_url = "https://www.jefit.com/exercises"
        self.all_exercises = []
        self.seen_links = set()

    def _init_driver(self):
        options = Options()
        options.add_argument("--headless=new")
        options.add_argument(f"--user-data-dir={tempfile.mkdtemp()}")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        return webdriver.Chrome(options=options)

    def open_page(self):
        print("üåê Opening Jefit exercises page...")
        self.driver.get(self.base_url)
        time.sleep(5)

    def scroll_to_load_all(self, scroll_count=12, delay=2):
        for _ in range(scroll_count):
            self.driver.execute_script("window.scrollBy(0, document.body.scrollHeight);")
            time.sleep(delay)

    def get_equipment_button(self, equip_name):
        buttons = self.driver.find_elements(By.XPATH, "//button[.//p]")
        for button in buttons:
            try:
                label = button.find_element(By.TAG_NAME, "p").text.strip()
                if label == equip_name:
                    return button
            except:
                continue
        return None

    def extract_exercise_data(self, card):
        try:
            name = " ".join(card.text.strip().split())  # Clean name
            link = card.get_attribute("href")
            exercise_id = link.rstrip("/").split("/")[-1]
            return name, link, exercise_id
        except:
            return None, None, None

    def scrape_equipment(self, equip_name):
        print(f"\n‚û°Ô∏è Scraping exercises for: {equip_name}")
        button = self.get_equipment_button(equip_name)
        if not button:
            print(f"‚ùå Equipment '{equip_name}' not found.")
            return

        try:
            button.click()
            time.sleep(3)
            self.scroll_to_load_all()

            cards = self.driver.find_elements(By.CSS_SELECTOR, "a[href*='/exercises/']")
            print(f"  ‚úÖ Found {len(cards)} cards.")

            for card in cards:
                name, link, exercise_id = self.extract_exercise_data(card)
                if name and link and link not in self.seen_links:
                    self.all_exercises.append({
                        "equipment": equip_name,
                        "exercise_id": exercise_id,
                        "name": name,
                        "link": link
                    })
                    self.seen_links.add(link)

            # Deselect the equipment filter
            button.click()
            time.sleep(1)

        except Exception as e:
            print(f"  ‚ùå Error scraping '{equip_name}': {e}")

    def scrape_all(self):
        self.open_page()
        for equip in self.equipment_names:
            self.scrape_equipment(equip)
        self.driver.quit()

    def save_to_csv(self, filename="jefit_exercises_by_equipment.csv"):
        df = pd.DataFrame(self.all_exercises)
        df['name'] = df['name'].str.replace(r'\s+', ' ', regex=True).str.strip()
        df.drop_duplicates(subset='link', inplace=True)
        df.to_csv(filename, index=False)
        print(f"\n‚úÖ DONE. Saved {len(df)} clean exercises to '{filename}'.")

# Run the scraper
if __name__ == "__main__":
    scraper = JefitScraper()
    scraper.scrape_all()
    scraper.save_to_csv()

"""# Merging datasets"""

import pandas as pd

class DataLoader:
    """
    Handles loading of exercise data from CSV files.
    """
    def __init__(self, equipment_path, muscle_path):
        self.equipment_path = equipment_path
        self.muscle_path = muscle_path

    def load_data(self):
        """
        Loads both equipment and muscle group datasets.
        """
        print("üì• Loading data...")
        df_equipment = pd.read_csv(self.equipment_path)
        df_muscle = pd.read_csv(self.muscle_path)
        print(f"  ‚úÖ Loaded {len(df_equipment)} equipment records.")
        print(f"  ‚úÖ Loaded {len(df_muscle)} muscle group records.")
        return df_equipment, df_muscle


class DataCleaner:
    """
    Handles merging, cleaning, and exporting of exercise datasets.
    """
    def __init__(self, df_equipment, df_muscle):
        self.df_equipment = df_equipment
        self.df_muscle = df_muscle
        self.cleaned_df = None

    def merge_and_clean(self):
        """
        Merges and cleans the datasets.
        """
        print("üßº Merging and cleaning datasets...")
        merged_df = pd.merge(self.df_equipment, self.df_muscle, on="name", how="inner")

        # Rename columns for clarity
        merged_df = merged_df.rename(columns={
            "exercise_id_x": "exercise_id",
            "link_x": "link"
        })

        # Drop duplicate columns
        merged_df = merged_df.drop(columns=["exercise_id_y", "link_y"])

        # Reorder for readability
        merged_df = merged_df[["exercise_id", "name", "muscle_group", "equipment", "link"]]

        self.cleaned_df = merged_df
        print(f"  ‚úÖ Cleaned dataset has {len(self.cleaned_df)} unique exercises.")
        return self.cleaned_df

    def save_to_csv(self, path="cleaned_exercises.csv"):
        """
        Saves the cleaned DataFrame to a CSV file.
        """
        if self.cleaned_df is not None:
            self.cleaned_df.to_csv(path, index=False)
            print(f"üíæ Saved cleaned data to: {path}")
        else:
            print("‚ö†Ô∏è No cleaned data to save. Run `merge_and_clean()` first.")


# ---- Main Usage Example ----
if __name__ == "__main__":
    equipment_path = "/content/jefit_exercises_by_equipment.csv"
    muscle_path = "/content/jefit_exercises_by_muscle_cleaned.csv"

    # Load data
    loader = DataLoader(equipment_path, muscle_path)
    df_equipment, df_muscle = loader.load_data()

    # Clean & export
    cleaner = DataCleaner(df_equipment, df_muscle)
    cleaner.merge_and_clean()
    cleaner.save_to_csv("final_cleaned_exercises.csv")

"""# Preprocessing"""

# Prefer exercise_id_x and link_x (from equipment file), drop the redundant ones
exercices_df = exercices_df.rename(columns={
    "exercise_id_x": "exercise_id",
    "link_x": "link"
})

# Drop unnecessary columns
exercises_df = exercices_df.drop(columns=["exercise_id_y", "link_y"])

# Optional: re-order columns
exercises_df = exercises_df[["exercise_id", "name", "muscle_group", "equipment", "link"]]

exercises_df["text"] = (
    "Exercise: " + exercises_df["name"] +
    "\nMuscle Group: " + exercises_df["muscle_group"] +
    "\nEquipment: " + exercises_df["equipment"] +
    "\nMore Info: " + exercises_df["link"]
)


import pandas as pd

# Load both CSVs
df1 = pd.read_csv("/content/jefit_exercises_by_equipment.csv")
df2 = pd.read_csv("/content/jefit_exercises_by_muscle_cleaned.csv")

# Merge on 'exercise_id'
exercises_df = pd.merge(df1, df2, on='exercise_id', how='inner')

# Clean columns
exercises_df = exercises_df.rename(columns={
    "exercise_id": "id",
    "link_x": "link",
    "name_x": "name"  # <- Rename here
})
exercises_df = exercises_df.drop(columns=["link_y", "name_y"])

exercises_df = exercises_df[["id", "name", "muscle_group", "equipment", "link"]]

# Create a semantic text field
exercises_df["text"] = (
    "Exercise: " + exercises_df["name"] +
    "\nMuscle Group: " + exercises_df["muscle_group"] +
    "\nEquipment: " + exercises_df["equipment"] +
    "\nMore Info: " + exercises_df["link"]
)

from sentence_transformers import SentenceTransformer
from langchain.vectorstores import FAISS
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.schema import Document

# Load the embedding model
embedding_model = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# Convert to LangChain Document format
docs = [
    Document(page_content=row["text"], metadata={"name": row["name"]})
    for _, row in exercises_df.iterrows()
]

# Create the FAISS index
vectorstore = FAISS.from_documents(docs, embedding_model)

from langchain_community.llms.ollama import Ollama

llm = Ollama(
    model="llama2",  # Or another model you have (e.g., mistral, phi3, etc.)
    base_url="http://localhost:11434"
)

from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

retriever = vectorstore.as_retriever()
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

rag_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=retriever,
    memory=memory,
)

query = "What are good exercises for chest using a barbell?"
response = rag_chain.run({"question": query})
print(response)