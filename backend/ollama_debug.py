# ollama_debug.py - Debug script to test Ollama connection
import requests
import json
import sys

def test_ollama_connection():
    """Test Ollama connection and diagnose issues"""
    
    base_url = "http://localhost:11434"
    
    print("ğŸ” Testing Ollama Connection...")
    print(f"Base URL: {base_url}")
    print("-" * 50)
    
    # Test 1: Check if Ollama server is running
    try:
        response = requests.get(f"{base_url}/api/tags", timeout=5)
        if response.status_code == 200:
            print("âœ… Ollama server is running")
            models = response.json()
            print(f"ğŸ“‹ Available models: {[model['name'] for model in models['models']]}")
        else:
            print(f"âŒ Ollama server responded with status {response.status_code}")
            return False
    except requests.exceptions.ConnectionError:
        print("âŒ Cannot connect to Ollama server")
        print("ğŸ’¡ Make sure Ollama is installed and running:")
        print("   - Install: curl -fsSL https://ollama.ai/install.sh | sh")
        print("   - Start: ollama serve")
        print("   - Pull model: ollama pull llama3.2:3b-instruct")
        return False
    except Exception as e:
        print(f"âŒ Connection error: {e}")
        return False
    
    # Test 2: Test the specific model you're using
    model_name = "llama3.2:3b-instruct"
    print(f"\nğŸ§ª Testing model: {model_name}")
    
    # Check if model exists
    try:
        models_response = requests.get(f"{base_url}/api/tags")
        available_models = [m['name'] for m in models_response.json()['models']]
        
        if model_name not in available_models:
            print(f"âŒ Model '{model_name}' not found")
            print(f"ğŸ“‹ Available models: {available_models}")
            print(f"ğŸ’¡ Pull the model: ollama pull {model_name}")
            return False
        else:
            print(f"âœ… Model '{model_name}' is available")
    except Exception as e:
        print(f"âŒ Error checking models: {e}")
        return False
    
    # Test 3: Test chat endpoint
    print(f"\nğŸ—£ï¸ Testing /api/chat endpoint...")
    
    chat_payload = {
        "model": model_name,
        "messages": [
            {"role": "user", "content": "Hello, are you working?"}
        ],
        "stream": False,
        "options": {
            "temperature": 0.7,
            "num_predict": 50
        }
    }
    
    try:
        response = requests.post(
            f"{base_url}/api/chat",
            json=chat_payload,
            headers={"Content-Type": "application/json"},
            timeout=30
        )
        
        if response.status_code == 200:
            data = response.json()
            if "message" in data and "content" in data["message"]:
                print("âœ… Chat endpoint working")
                print(f"ğŸ“ Response: {data['message']['content'][:100]}...")
            else:
                print("âŒ Unexpected chat response format")
                print(f"ğŸ“ Response: {response.text}")
        elif response.status_code == 404:
            print("âŒ Chat endpoint not supported, trying /api/generate...")
            return test_generate_endpoint(base_url, model_name)
        else:
            print(f"âŒ Chat endpoint failed with status {response.status_code}")
            print(f"ğŸ“ Response: {response.text}")
            return False
            
    except requests.exceptions.Timeout:
        print("âŒ Chat request timed out (model might be loading)")
        print("ğŸ’¡ Wait a moment and try again")
        return False
    except Exception as e:
        print(f"âŒ Chat endpoint error: {e}")
        return False
    
    print("\nğŸ‰ All tests passed! Ollama is working correctly.")
    return True

def test_generate_endpoint(base_url, model_name):
    """Test the generate endpoint as fallback"""
    
    gen_payload = {
        "model": model_name,
        "prompt": "Hello, are you working?",
        "stream": False,
        "options": {
            "temperature": 0.7,
            "num_predict": 50
        }
    }
    
    try:
        response = requests.post(
            f"{base_url}/api/generate",
            json=gen_payload,
            headers={"Content-Type": "application/json"},
            timeout=30
        )
        
        if response.status_code == 200:
            data = response.json()
            if "response" in data:
                print("âœ… Generate endpoint working")
                print(f"ğŸ“ Response: {data['response'][:100]}...")
                return True
            else:
                print("âŒ Unexpected generate response format")
                print(f"ğŸ“ Response: {response.text}")
        else:
            print(f"âŒ Generate endpoint failed with status {response.status_code}")
            print(f"ğŸ“ Response: {response.text}")
            
    except Exception as e:
        print(f"âŒ Generate endpoint error: {e}")
    
    return False

def test_llm_engine():
    """Test your LLMEngine class specifically"""
    print("\nğŸ§ª Testing your LLMEngine class...")
    
    try:
        # Import your LLMEngine (adjust path as needed)
        sys.path.append('.')  # Add current directory to path
        from services.llm_engine import LLMEngine
        
        # Initialize with Ollama
        llm = LLMEngine(provider="ollama", model="llama3.2:3b-instruct")
        
        # Test invoke
        response = llm.invoke("Say hello in one sentence.")
        print(f"âœ… LLMEngine working")
        print(f"ğŸ“ Response: {response[:100]}...")
        
    except ImportError as e:
        print(f"âŒ Cannot import LLMEngine: {e}")
        print("ğŸ’¡ Make sure the path to your LLMEngine is correct")
    except Exception as e:
        print(f"âŒ LLMEngine error: {e}")

if __name__ == "__main__":
    print("ğŸš€ Ollama Debug Tool")
    print("=" * 50)
    
    if test_ollama_connection():
        test_llm_engine()
    
    print("\n" + "=" * 50)
    print("Debug complete!")